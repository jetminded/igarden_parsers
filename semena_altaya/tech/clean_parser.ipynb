{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujHcik3UZR7M"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse, urldefrag, urljoin\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from queue import Queue\n",
        "import time as time\n",
        "\n",
        "pip install url_normalize\n",
        "\n",
        "from url_normalize import url_normalize\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_from_the_internet(url):\n",
        "    '''Скачивает сраницу с интернета\n",
        "\n",
        "    Параметры:\n",
        "        url (str) - ссылка на страницу для скачивания\n",
        "\n",
        "    Возвращает:\n",
        "        str - html-страница в виде строки, None в случае неудачи\n",
        "    '''\n",
        "    try:\n",
        "        return urlopen(url).read().decode('utf-8')\n",
        "    except KeyboardInterrupt:\n",
        "        raise\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    \n",
        "def extract_links_from_html(url, html):\n",
        "    '''Парсит ссылки на странице\n",
        "\n",
        "    Принимает:\n",
        "        url (str) - исходный урл страницы\n",
        "        html (str) - содержание html-страницы\n",
        "\n",
        "    Возвращает:\n",
        "        list - список ссылок, находящихся на странице\n",
        "    '''\n",
        "    parser = BeautifulSoup(html)\n",
        "    # Формируем ссылки на те страницы, на которые ссылается документ\n",
        "    return [urljoin(url, link.get('href')) for link in parser.findAll('a')]\n",
        "\n",
        "\n",
        "def extract_text_info_from_html(html):\n",
        "    '''Парсит текстовую информацию на странице\n",
        "\n",
        "    Принимает:\n",
        "         html (str) - содержание html-страницы\n",
        "\n",
        "    Возвращает:\n",
        "        dict - текстовая часть страницы по ключу text,\n",
        "               название по ключу title\n",
        "    '''\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()\n",
        "    \n",
        "    # Объединяем строки текста\n",
        "    text = soup.get_text()\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    # Находим название на странице\n",
        "    title = soup.find('title').string\n",
        "    \n",
        "    return {'text': text, 'title': title}"
      ],
      "metadata": {
        "id": "aLnYnFi5anLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_web_pages(seed, max_downloads, check_link, filtration_function, adjust_function):\n",
        "    '''Обходит web-страницы в ширину и загружает информацию о них.\n",
        "    \n",
        "    Принимает:\n",
        "        seed (str) -- страница, с которой начинать обход.\n",
        "        max_downloads (int) -- максимальное число загруженных страниц.\n",
        "        filtration_function (str, str -> bool) -- функция, указывающая, \n",
        "            стоит ли загружать страницу. Пример: is_wiki_article.\n",
        "        check_link (str) -- ссылка, по которой работает ф-ия фильтрации\n",
        "        adjust_function (dict -> dict) - функция, которая фиксит то, что\n",
        "                                        вытянули со странички\n",
        "            \n",
        "    Возвращает:\n",
        "        pages_json (list) - список словарей с информацией о страницах.\n",
        "    '''\n",
        "    \n",
        "    # Создаём список со страницами\n",
        "    pages_json = []\n",
        "    \n",
        "    # Создаём очередь для обхода в ширину\n",
        "    q = Queue()\n",
        "    q.put(seed)\n",
        "\n",
        "    already_visited = set()\n",
        "    n_downloads = 0\n",
        "    time_start = time.time()\n",
        "\n",
        "    while not q.empty():\n",
        "        # Нормализуем урл\n",
        "        main_url = url_normalize(q.get())\n",
        "        if main_url in already_visited:\n",
        "            continue\n",
        "        already_visited.add(main_url)\n",
        "        html = download_from_the_internet(main_url)\n",
        "\n",
        "        # Извлекаем ссылки из страницы\n",
        "        if not(html is None):\n",
        "            children_links = extract_links_from_html(main_url, html)\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Извлекаем текст страницы\n",
        "            text_info = extract_text_info_from_html(html)\n",
        "            text_info['url'] = main_url\n",
        "\n",
        "            # Добавляем запись в таблицу\n",
        "            pages_json.append(adjust_function(text_info))\n",
        "\n",
        "            n_downloads += 1\n",
        "            if n_downloads > max_downloads:\n",
        "                break\n",
        "\n",
        "            # Добавляем ещё не посещённые ссылки в очередь\n",
        "            for child in children_links:\n",
        "                if url_normalize(child) not in already_visited \\\n",
        "                and filtration_function(check_link, child):\n",
        "                    q.put(child)\n",
        "                \n",
        "    return pages_json\n",
        "\n",
        "\n",
        "def is_article(link, url):\n",
        "    '''Проверяет, является ли ссылка нужной страницей'''\n",
        "    ''' На вход:    link - какое-то ключевое слово, по которому првоерка\n",
        "                    url - ссылка, которую проверяем'''\n",
        "    if (link == url):\n",
        "        return False\n",
        "    if (link in url):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def check_integer(string):\n",
        "  return any(map(str.isdigit, string))\n",
        "\n",
        "def rework_info(text_info):\n",
        "    '''\n",
        "    Фиксит то, что вытянулось в text_info\n",
        "    '''\n",
        "    text_parts = text_info['text'].split('\\n')\n",
        "    variety_info = dict() \n",
        "    variety_info['url'] = text_info['url']\n",
        "    variety_info['name'] = text_parts[56].split('/')[0]\n",
        "    variety_info['unique_text'] = text_parts[64]\n",
        "    return variety_info"
      ],
      "metadata": {
        "id": "gBt66-eHaoMb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# константы\n",
        "URL_SEED = \"https://altsemena.org/catalog/semena/ovoshchi-i-zelennye-kultury/tomat/\" # ссылка на каталог со страницами\n",
        "PAGES_SITE = 5 # кол-во страниц в общем каталоге\n",
        "MAX_PAGES = 170 # кол-во страниц в итоговом датасете"
      ],
      "metadata": {
        "id": "YkOJjxQna-1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(PAGES_SITE):\n",
        "  seed = ''\n",
        "  if i == 0:\n",
        "    seed = URL_SEED\n",
        "  else:\n",
        "    seed = URL_SEED + '?PAGEN_1' + str(i + 1)\n",
        "\n",
        "  json_data = load_web_pages(seed, MAX_PAGES, URL_SEED, is_article, rework_info)\n",
        "  new_df = pd.DataFrame(json_data)"
      ],
      "metadata": {
        "id": "NyoaKXdocedy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(10)"
      ],
      "metadata": {
        "id": "DaKbg625celb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.to_csv('seeds_df.csv')\n",
        "files.download('seeds_df.csv')"
      ],
      "metadata": {
        "id": "d3ngjh_GdXkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}