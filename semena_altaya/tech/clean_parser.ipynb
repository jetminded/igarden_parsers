{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ujHcik3UZR7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: url_normalize in /home/artemy/.local/lib/python3.8/site-packages (1.4.3)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from url_normalize) (1.14.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install url_normalize\n",
    "\n",
    "from urllib.parse import urlparse, urldefrag, urljoin\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "import time as time\n",
    "\n",
    "from url_normalize import url_normalize\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def download_from_the_internet(url):\n",
    "    '''Скачивает сраницу с интернета\n",
    "\n",
    "    Параметры:\n",
    "        url (str) - ссылка на страницу для скачивания\n",
    "\n",
    "    Возвращает:\n",
    "        str - html-страница в виде строки, None в случае неудачи\n",
    "    '''\n",
    "    try:\n",
    "        return urlopen(url).read().decode('utf-8')\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def extract_links_from_html(url, html):\n",
    "    '''Парсит ссылки на странице\n",
    "\n",
    "    Принимает:\n",
    "        url (str) - исходный урл страницы\n",
    "        html (str) - содержание html-страницы\n",
    "\n",
    "    Возвращает:\n",
    "        list - список ссылок, находящихся на странице\n",
    "    '''\n",
    "    parser = BeautifulSoup(html)\n",
    "    # Формируем ссылки на те страницы, на которые ссылается документ\n",
    "    return [urljoin(url, link.get('href')) for link in parser.findAll('a')]\n",
    "\n",
    "\n",
    "def extract_text_info_from_html(html):\n",
    "    '''Парсит текстовую информацию на странице\n",
    "\n",
    "    Принимает:\n",
    "         html (str) - содержание html-страницы\n",
    "\n",
    "    Возвращает:\n",
    "        dict - текстовая часть страницы по ключу text,\n",
    "               название по ключу title\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    # Объединяем строки текста\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    # Находим название на странице\n",
    "    title = soup.find('title').string\n",
    "    \n",
    "    return {'text': text, 'title': title}"
   ],
   "metadata": {
    "id": "aLnYnFi5anLU"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_web_pages(seed, max_downloads, check_link, filtration_function, adjust_function):\n",
    "    '''Обходит web-страницы в ширину и загружает информацию о них.\n",
    "    \n",
    "    Принимает:\n",
    "        seed (str) -- страница, с которой начинать обход.\n",
    "        max_downloads (int) -- максимальное число загруженных страниц.\n",
    "        filtration_function (str, str -> bool) -- функция, указывающая, \n",
    "            стоит ли загружать страницу. Пример: is_wiki_article.\n",
    "        check_link (str) -- ссылка, по которой работает ф-ия фильтрации\n",
    "        adjust_function (dict -> dict) - функция, которая фиксит то, что\n",
    "                                        вытянули со странички\n",
    "            \n",
    "    Возвращает:\n",
    "        pages_json (list) - список словарей с информацией о страницах.\n",
    "    '''\n",
    "    \n",
    "    # Создаём список со страницами\n",
    "    pages_json = []\n",
    "    \n",
    "    # Создаём очередь для обхода в ширину\n",
    "    q = Queue()\n",
    "    q.put(seed)\n",
    "\n",
    "    already_visited = set()\n",
    "    n_downloads = 0\n",
    "    time_start = time.time()\n",
    "\n",
    "    while not q.empty():\n",
    "        # Нормализуем урл\n",
    "        main_url = url_normalize(q.get())\n",
    "        if main_url in already_visited:\n",
    "            continue\n",
    "        already_visited.add(main_url)\n",
    "        html = download_from_the_internet(main_url)\n",
    "\n",
    "        # Извлекаем ссылки из страницы\n",
    "        if not(html is None):\n",
    "            children_links = extract_links_from_html(main_url, html)\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Извлекаем текст страницы\n",
    "            text_info = extract_text_info_from_html(html)\n",
    "            text_info['url'] = main_url\n",
    "\n",
    "            # Добавляем запись в таблицу\n",
    "            pages_json.append(adjust_function(text_info))\n",
    "\n",
    "            n_downloads += 1\n",
    "            if n_downloads > max_downloads:\n",
    "                break\n",
    "\n",
    "            # Добавляем ещё не посещённые ссылки в очередь\n",
    "            for child in children_links:\n",
    "                if url_normalize(child) not in already_visited \\\n",
    "                and filtration_function(check_link, child):\n",
    "                    q.put(child)\n",
    "        df = pd.DataFrame(pages_json)\n",
    "        df.head()\n",
    "        df.to_csv(f'seeds_df_{plant_name}.csv')\n",
    "    return pages_json\n",
    "\n",
    "\n",
    "def is_article(link, url):\n",
    "    '''Проверяет, является ли ссылка нужной страницей'''\n",
    "    ''' На вход:    link - какое-то ключевое слово, по которому првоерка\n",
    "                    url - ссылка, которую проверяем'''\n",
    "    if (link == url):\n",
    "        return False\n",
    "    if (link in url):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_integer(string):\n",
    "  return any(map(str.isdigit, string))\n",
    "\n",
    "def rework_info(text_info):\n",
    "    '''\n",
    "    Фиксит то, что вытянулось в text_info\n",
    "    '''\n",
    "    text_parts = text_info['text'].split('\\n')\n",
    "    variety_info = dict() \n",
    "    variety_info['url'] = text_info['url']\n",
    "    variety_info['name'] = text_parts[56].split('/')[0]\n",
    "    variety_info['unique_text'] = text_parts[64]\n",
    "    return variety_info"
   ],
   "metadata": {
    "id": "gBt66-eHaoMb"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# константы\n",
    "plant_name = 'tomat'\n",
    "URL_SEED = f\"https://altsemena.org/catalog/semena/ovoshchi-i-zelennye-kultury/{plant_name}/\" # ссылка на каталог со страницами\n",
    "PAGES_SITE = 5 # кол-во страниц в общем каталоге\n",
    "MAX_PAGES = 170 # кол-во страниц в итоговом датасете"
   ],
   "metadata": {
    "id": "YkOJjxQna-1I"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(PAGES_SITE):\n",
    "  seed = ''\n",
    "  if i == 0:\n",
    "    seed = URL_SEED\n",
    "  else:\n",
    "    seed = URL_SEED + '?PAGEN_1' + str(i + 1)\n",
    "\n",
    "  json_data = load_web_pages(seed, MAX_PAGES, URL_SEED, is_article, rework_info)\n",
    "  new_df = pd.DataFrame(json_data)"
   ],
   "metadata": {
    "id": "NyoaKXdocedy"
   },
   "execution_count": 10,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-e5354e562a92>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mseed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mURL_SEED\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'?PAGEN_1'\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m   \u001B[0mjson_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_web_pages\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMAX_PAGES\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mURL_SEED\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_article\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrework_info\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m   \u001B[0mnew_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-907a2c277412>\u001B[0m in \u001B[0;36mload_web_pages\u001B[0;34m(seed, max_downloads, check_link, filtration_function, adjust_function)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhtml\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mchildren_links\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_links_from_html\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmain_url\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhtml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m             \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0;31m# Извлекаем текст страницы\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "new_df.head(10)"
   ],
   "metadata": {
    "id": "DaKbg625celb",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_df.to_csv(f'seeds_df_{plant_name}.csv')\n",
    "# files.download('seeds_df.csv')"
   ],
   "metadata": {
    "id": "d3ngjh_GdXkC",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
